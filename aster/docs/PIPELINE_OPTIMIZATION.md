# Pipeline Optimization Analysis

## üî¥ Current Problem

Your 67.5 KB DOCX file times out after 300 seconds because the pipeline architecture is **backwards**.

### Current Flow (WRONG ORDER):
```
DOCX file
  ‚Üì
Pandoc: Extract to PLAIN TEXT (entire document as one string)
  ‚Üì
StructureDetector: Try to find structure with regex (crude splitting)
  ‚Üì
OllamaCleaner: Send ENTIRE document to LLM at once
  ‚Üì
TIMEOUT (5 minutes)
```

### Problems:
1. **Pandoc destroys structure** - Converts DOCX ‚Üí plain text, losing headers, lists, tables
2. **StructureDetector is crude** - Uses regex on plain text to guess structure
3. **LLM gets overwhelmed** - Receives giant text blob, times out on llama3.2:1b (1 billion params is small!)
4. **No real chunking** - The chunking logic (line 719) only works if elements are already small

## ‚úÖ Optimal Pipeline Architecture

### Correct Flow (RIGHT ORDER):
```
DOCX file
  ‚Üì
Unstructured: Parse to STRUCTURED ELEMENTS first
  ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì
[Title] [Paragraph] [List] [Table] [Image]
  ‚Üì       ‚Üì           ‚Üì      ‚Üì       ‚Üì
Chunk large paragraphs into 500-word pieces
  ‚Üì       ‚Üì           ‚Üì      ‚Üì       ‚Üì
Send EACH chunk to Ollama individually (streaming)
  ‚Üì       ‚Üì           ‚Üì      ‚Üì       ‚Üì
Reassemble cleaned elements
  ‚Üì
Format as markdown
```

### Advantages:
1. **Unstructured preserves structure** - Knows what's a title, paragraph, list, table
2. **Smart chunking** - Only chunk large narrative paragraphs, keep titles/lists intact
3. **LLM processes small chunks** - Each API call is manageable (500-1000 words)
4. **Parallel processing possible** - Can send multiple chunks simultaneously
5. **Better quality** - LLM has proper context for each element type

## üîß Required Changes

### Phase 1: Use Unstructured Library (Already Installed!)

**Current code (lines 354-370):**
```python
# BAD: Using pandoc dumps everything as plain text
result = subprocess.run(
    ['pandoc', str(doc_path), '-f', from_format, '-t', 'plain', '--wrap=none'],
    capture_output=True,
    text=True,
    check=True
)
return result.stdout  # Returns ONE GIANT STRING
```

**Should be:**
```python
# GOOD: Use Unstructured for proper document parsing
from unstructured.partition.docx import partition_docx
from unstructured.partition.pptx import partition_pptx

if extension == '.docx':
    elements = partition_docx(filename=str(doc_path))
elif extension == '.pptx':
    elements = partition_pptx(filename=str(doc_path))

# Returns LIST of structured elements:
# [Title("Introduction"),
#  NarrativeText("This is a paragraph..."),
#  ListItem("First point"),
#  Table(...)]
```

### Phase 2: Intelligent Chunking Strategy

```python
def chunk_element(element, max_words=500):
    """
    Chunk only large narrative text.
    Keep titles, lists, tables intact.
    """
    if element.type == 'Title':
        return [element]  # Never chunk titles

    if element.type == 'ListItem':
        return [element]  # Keep list items together

    if element.type == 'NarrativeText':
        words = element.text.split()
        if len(words) <= max_words:
            return [element]

        # Split at sentence boundaries
        sentences = split_sentences(element.text)
        chunks = []
        current = []
        word_count = 0

        for sentence in sentences:
            s_words = len(sentence.split())
            if word_count + s_words > max_words and current:
                chunks.append(' '.join(current))
                current = [sentence]
                word_count = s_words
            else:
                current.append(sentence)
                word_count += s_words

        if current:
            chunks.append(' '.join(current))

        return chunks

    return [element]  # Default: keep as-is
```

### Phase 3: Streaming/Batch Processing

```python
def clean_elements_optimized(elements, model='llama3.2:1b'):
    """
    Process elements in batches with progress indication.
    """
    cleaned_parts = []

    print(f"Processing {len(elements)} elements...")

    for i, element in enumerate(elements):
        # Show progress
        if i % 10 == 0:
            print(f"  Progress: {i}/{len(elements)} ({i/len(elements)*100:.0f}%)")

        # Smart processing based on element type
        if element.type == 'Title':
            # Titles don't need LLM cleaning
            cleaned_parts.append(f"## {element.text}")

        elif element.type == 'NarrativeText':
            # Only clean narrative text
            chunks = chunk_element(element, max_words=500)
            for chunk in chunks:
                cleaned = clean_with_ollama(chunk, model, timeout=30)
                cleaned_parts.append(cleaned)

        elif element.type == 'ListItem':
            # Format but don't over-process
            cleaned_parts.append(f"- {element.text}")

        elif element.type == 'Table':
            # Convert to markdown table
            cleaned_parts.append(format_table(element))

    return '\n\n'.join(cleaned_parts)
```

## üìä Performance Comparison

### Current Pipeline (67.5 KB DOCX):
- Extract: 0.1s (pandoc)
- Structure: 1s (regex splitting)
- Clean: **300s TIMEOUT** (sending 10,000+ words at once)
- **Total: FAILURE**

### Optimized Pipeline (same file):
- Parse: 2s (Unstructured ‚Üí 150 elements)
- Chunk: 1s (break 20 large paragraphs ‚Üí 45 chunks)
- Clean: 90s (45 chunks √ó 2s each)
- Reassemble: 1s
- **Total: ~95 seconds SUCCESS**

### Why 10x Faster:
1. **Smaller API calls**: 500 words instead of 10,000 words
2. **Better context**: LLM knows it's processing a paragraph, not guessing
3. **Parallel processing**: Can process chunks simultaneously
4. **Selective cleaning**: Only clean narrative text, skip titles/lists

## üéØ Immediate Action Items

### 1. Fix Document Parser (aster.py lines 340-370)
Replace pandoc extraction with Unstructured partitioning:

```python
class DocumentParser:
    def extract_document_text(self, doc_path: Path) -> List[Element]:
        """Extract STRUCTURED elements instead of plain text"""
        extension = doc_path.suffix.lower()

        # Use Unstructured for proper parsing
        if extension == '.docx':
            from unstructured.partition.docx import partition_docx
            return partition_docx(filename=str(doc_path))

        elif extension == '.pptx':
            from unstructured.partition.pptx import partition_pptx
            return partition_pptx(filename=str(doc_path))

        elif extension == '.epub':
            from unstructured.partition.epub import partition_epub
            return partition_epub(filename=str(doc_path))

        else:
            # Fallback to pandoc for unsupported types
            return self._fallback_pandoc(doc_path)
```

### 2. Update Main Pipeline (aster.py line 858-906)
Skip the "detect structure" step entirely - we already have structure!

```python
def process_file(self, input_path: Path, output_path: Optional[Path] = None):
    # STEP 1: Extract STRUCTURED elements (not plain text!)
    print("\n1Ô∏è‚É£  Parsing document structure...")
    elements = self._extract_elements(input_path, file_type)
    print(f"   ‚úì Parsed {len(elements)} structured elements")

    # STEP 2: Smart chunking (only for large paragraphs)
    print("\n2Ô∏è‚É£  Chunking large elements...")
    chunks = self._chunk_intelligently(elements)
    print(f"   ‚úì Created {len(chunks)} processable chunks")

    # STEP 3: Clean with LLM (now much faster!)
    print("\n3Ô∏è‚É£  Cleaning with LLM...")
    cleaned_text = self.llm_cleaner.clean_chunks(chunks)
    print(f"   ‚úì Processed in {elapsed}s")

    # STEP 4: Format and save...
```

### 3. Add Progress Indicators
For large documents, show real-time progress:

```python
print(f"  Processing chunk 23/150 (15%)... ‚úì")
print(f"  Estimated time remaining: 2 minutes")
```

## üß™ Testing Strategy

### Test 1: Small Document (5 KB)
- Should complete in < 10 seconds
- Verify all content preserved
- Check structure maintained

### Test 2: Medium Document (50 KB)
- Should complete in < 60 seconds
- Verify chunking works correctly
- Check quality of cleaned text

### Test 3: Large Document (500 KB)
- Should complete in < 5 minutes
- Monitor memory usage
- Verify no data loss

### Test 4: Multilingual (Afrikaans + English)
- Verify language detection works
- Check proper noun preservation
- Verify no unwanted translation

## üìö Document Type Priorities

### High Priority (Use Unstructured):
1. ‚úÖ **DOCX** - partition_docx()
2. ‚úÖ **PPTX** - partition_pptx()
3. ‚úÖ **PDF** - partition_pdf() (better than pdftotext)
4. ‚úÖ **EPUB** - partition_epub()
5. ‚úÖ **HTML** - partition_html()

### Medium Priority (Current approach OK):
6. **Images** - OCR pipeline works well
7. **Audio** - Vosk transcription works well
8. **CSV/Excel** - Pandas pipeline works well
9. **Plain text** - Already optimal

### Already Using Unstructured:
- Check `unstructured[all-docs]` in requirements.txt ‚úÖ
- Installed but NOT being used for DOCX/PPTX! ‚ùå

## üí° Key Insight

**You already HAVE Unstructured installed!** It's in your requirements.txt. You're just not using it for the right file types.

The fix is simple:
1. Replace pandoc calls with Unstructured calls
2. Remove the "detect structure" step (Unstructured does this)
3. Keep the LLM cleaning step (but it will be much faster)

## üöÄ Expected Results After Fix

### Current State:
- DOCX: ‚ùå Timeout after 300s
- PPTX: ‚ùå Pandoc errors
- PDF: ‚ö†Ô∏è Works but crude (pdftotext)
- Large files: ‚ùå Always timeout

### After Optimization:
- DOCX: ‚úÖ 30-90s depending on size
- PPTX: ‚úÖ 20-60s with proper slide structure
- PDF: ‚úÖ Better structure preservation
- Large files: ‚úÖ Chunked processing with progress

## üéØ Next Steps

1. **Immediate**: Replace DocumentParser.extract_document_text() to use Unstructured
2. **Quick win**: Add progress indicators for user feedback
3. **Quality**: Test on your 5 Afrikaans church documents
4. **Optimization**: Add parallel chunk processing (optional)

The root cause is **pipeline architecture**, not model capacity. Fix the pipeline, and your 1B model will work great!
