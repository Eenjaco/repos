# Advanced Pipeline Optimizations

## Current Status ‚úÖ

### Already Optimized (Latest Commit):
1. ‚úÖ **DOCX** - Uses Unstructured library for structured parsing
2. ‚úÖ **PPTX** - Uses Unstructured library for slide structure
3. ‚úÖ **PDF** - Uses Unstructured library (fallback to pdftotext)
4. ‚úÖ **HTML** - Uses Unstructured library (fallback to pandoc)
5. ‚úÖ **EPUB** - Uses Unstructured library

### Current Implementation (Works But Can Be Improved):
6. ‚ö†Ô∏è **Images** - Uses pytesseract OCR (no preprocessing)
7. ‚ö†Ô∏è **Audio** - Uses Vosk transcription (sequential, no chunking)
8. ‚ö†Ô∏è **Scanned PDFs** - Uses Ghostscript + OCR (no image enhancement)

---

## üéØ Optimization Strategy by File Type

### 1. Images (JPG, PNG, TIFF)

#### Current Flow:
```
Image ‚Üí pytesseract OCR ‚Üí Raw text ‚Üí LLM cleanup
```

#### Problem:
- Low contrast images = poor OCR accuracy
- Skewed images = missed text
- Noisy backgrounds = false characters
- Low resolution = gibberish output

#### Optimized Flow:
```
Image
  ‚Üì
Auto-enhance (contrast, brightness, sharpness)
  ‚Üì
Convert to high-contrast B&W
  ‚Üì
Deskew if rotated
  ‚Üì
Upscale if resolution < 300 DPI
  ‚Üì
Remove noise/speckles
  ‚Üì
pytesseract OCR (now much more accurate!)
  ‚Üì
Raw text ‚Üí LLM cleanup
```

#### Tools Needed:
- **Pillow** (already in Python) - Basic preprocessing
- **ImageMagick** (optional, installed) - Advanced preprocessing
- **opencv-python** (optional) - Deskewing, noise removal

#### Implementation:
```python
from PIL import Image, ImageEnhance, ImageFilter
import numpy as np

def preprocess_image_for_ocr(image_path: Path) -> Path:
    """
    Enhance image for better OCR accuracy.
    Returns path to preprocessed image.
    """
    img = Image.open(image_path)

    # Step 1: Convert to grayscale
    img = img.convert('L')

    # Step 2: Increase contrast
    enhancer = ImageEnhance.Contrast(img)
    img = enhancer.enhance(2.0)  # 2x contrast

    # Step 3: Increase sharpness
    enhancer = ImageEnhance.Sharpness(img)
    img = enhancer.enhance(2.0)

    # Step 4: Convert to pure black & white (threshold)
    threshold = 128
    img = img.point(lambda p: 255 if p > threshold else 0)

    # Step 5: Remove noise
    img = img.filter(ImageFilter.MedianFilter(size=3))

    # Step 6: Upscale if too small
    if img.width < 1000:
        scale_factor = 1000 / img.width
        new_size = (int(img.width * scale_factor),
                   int(img.height * scale_factor))
        img = img.resize(new_size, Image.LANCZOS)

    # Save preprocessed image
    temp_path = image_path.parent / f"preprocessed_{image_path.name}"
    img.save(temp_path)

    return temp_path
```

#### Expected Improvement:
- OCR accuracy: 70% ‚Üí 95% on challenging images
- Processing time: +2s preprocessing, but worth it for quality

---

### 2. Audio Files (MP3, M4A, WMA, WAV, FLAC)

#### Current Flow:
```
Audio file ‚Üí Check format ‚Üí Convert to WAV if needed ‚Üí Vosk transcription (entire file) ‚Üí Text
```

#### Problems:
1. **Large files timeout** - 50 MB+ audio files take 10+ minutes
2. **Sequential processing** - One file at a time
3. **Memory intensive** - Loading entire audio file into RAM
4. **No progress feedback** - User has no idea how long it will take
5. **Format inconsistency** - Some formats fail to convert

#### Optimized Flow:
```
Audio file
  ‚Üì
Normalize format (convert all to WAV with standard settings)
  ‚Üì
Detect duration
  ‚Üì
If > 10 minutes: Split into 5-minute chunks
  ‚Üì
Queue all chunks for background transcription
  ‚Üì
Transcribe chunks in PARALLEL (multiple workers)
  ‚Üì
Show progress: "Chunk 3/8 (37%)"
  ‚Üì
Reassemble transcripts with timestamps
  ‚Üì
Pass through LLM cleanup pipeline
```

#### Tools Needed:
- **ffmpeg** (already installed) - Audio manipulation
- **pydub** (optional) - Python audio library
- **multiprocessing** or **asyncio** - Parallel processing

#### Implementation:

##### A. Audio Format Normalization
```python
def normalize_audio(audio_path: Path, output_path: Path) -> Path:
    """
    Convert any audio format to standard WAV for Vosk.
    - 16kHz sample rate (Vosk requirement)
    - Mono channel
    - 16-bit PCM
    """
    import subprocess

    cmd = [
        'ffmpeg',
        '-i', str(audio_path),
        '-ar', '16000',        # 16kHz sample rate
        '-ac', '1',            # Mono
        '-sample_fmt', 's16',  # 16-bit
        '-y',                  # Overwrite
        str(output_path)
    ]

    subprocess.run(cmd, capture_output=True, check=True)
    return output_path
```

##### B. Audio Chunking
```python
def split_audio_into_chunks(audio_path: Path,
                           chunk_duration_minutes: int = 5) -> List[Path]:
    """
    Split large audio file into smaller chunks.
    """
    import subprocess
    import json

    # Get audio duration
    probe_cmd = [
        'ffprobe',
        '-v', 'quiet',
        '-print_format', 'json',
        '-show_format',
        str(audio_path)
    ]

    result = subprocess.run(probe_cmd, capture_output=True, text=True)
    duration = float(json.loads(result.stdout)['format']['duration'])

    # Calculate number of chunks
    chunk_seconds = chunk_duration_minutes * 60
    num_chunks = int(duration / chunk_seconds) + 1

    chunks = []
    chunk_dir = audio_path.parent / f"{audio_path.stem}_chunks"
    chunk_dir.mkdir(exist_ok=True)

    for i in range(num_chunks):
        start_time = i * chunk_seconds
        chunk_path = chunk_dir / f"chunk_{i:03d}.wav"

        # Extract chunk
        cmd = [
            'ffmpeg',
            '-i', str(audio_path),
            '-ss', str(start_time),
            '-t', str(chunk_seconds),
            '-ar', '16000',
            '-ac', '1',
            '-y',
            str(chunk_path)
        ]

        subprocess.run(cmd, capture_output=True, check=True)
        chunks.append((chunk_path, start_time))

    return chunks
```

##### C. Parallel Transcription
```python
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass

@dataclass
class TranscriptChunk:
    chunk_number: int
    start_time: float
    text: str

def transcribe_chunk(chunk_path: Path, start_time: float,
                     chunk_number: int) -> TranscriptChunk:
    """Transcribe a single audio chunk."""
    # Use Vosk for transcription
    from vosk import Model, KaldiRecognizer
    import wave

    model = Model(model_name="vosk-model-small-en-us-0.15")

    wf = wave.open(str(chunk_path), "rb")
    rec = KaldiRecognizer(model, wf.getframerate())

    transcripts = []
    while True:
        data = wf.readframes(4000)
        if len(data) == 0:
            break
        if rec.AcceptWaveform(data):
            result = json.loads(rec.Result())
            transcripts.append(result.get('text', ''))

    # Final result
    result = json.loads(rec.FinalResult())
    transcripts.append(result.get('text', ''))

    full_text = ' '.join(transcripts)

    return TranscriptChunk(
        chunk_number=chunk_number,
        start_time=start_time,
        text=full_text
    )

def transcribe_audio_parallel(chunks: List[Tuple[Path, float]],
                              max_workers: int = 4) -> str:
    """
    Transcribe audio chunks in parallel.
    Returns assembled transcript.
    """
    results = []

    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # Submit all chunks
        futures = {
            executor.submit(transcribe_chunk, chunk, start_time, i): i
            for i, (chunk, start_time) in enumerate(chunks)
        }

        # Process as they complete
        for future in as_completed(futures):
            chunk_num = futures[future]
            try:
                result = future.result()
                results.append(result)
                print(f"  Chunk {chunk_num + 1}/{len(chunks)} complete ‚úì")
            except Exception as e:
                print(f"  Chunk {chunk_num} failed: {e}")

    # Sort by chunk number and assemble
    results.sort(key=lambda x: x.chunk_number)

    # Add timestamps and combine
    full_transcript = []
    for chunk in results:
        time_marker = format_timestamp(chunk.start_time)
        full_transcript.append(f"[{time_marker}] {chunk.text}")

    return '\n\n'.join(full_transcript)

def format_timestamp(seconds: float) -> str:
    """Format seconds as HH:MM:SS"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}"
```

##### D. Background Queue System
```python
from queue import Queue
from threading import Thread

class AudioTranscriptionQueue:
    """Background queue for long-running audio transcription"""

    def __init__(self, max_workers: int = 2):
        self.queue = Queue()
        self.results = {}
        self.max_workers = max_workers
        self.workers = []
        self._start_workers()

    def _start_workers(self):
        """Start background worker threads"""
        for i in range(self.max_workers):
            worker = Thread(target=self._worker, daemon=True)
            worker.start()
            self.workers.append(worker)

    def _worker(self):
        """Worker thread that processes queued audio files"""
        while True:
            job = self.queue.get()
            if job is None:
                break

            try:
                file_id, audio_path, callback = job
                print(f"[Background] Transcribing {audio_path.name}...")

                # Process audio
                chunks = split_audio_into_chunks(audio_path)
                transcript = transcribe_audio_parallel(chunks)

                # Store result
                self.results[file_id] = {
                    'status': 'complete',
                    'transcript': transcript
                }

                # Call callback if provided
                if callback:
                    callback(file_id, transcript)

                print(f"[Background] {audio_path.name} complete ‚úì")

            except Exception as e:
                self.results[file_id] = {
                    'status': 'failed',
                    'error': str(e)
                }

            finally:
                self.queue.task_done()

    def add_audio_file(self, audio_path: Path,
                       callback=None) -> str:
        """Add audio file to transcription queue"""
        import uuid
        file_id = str(uuid.uuid4())

        self.results[file_id] = {'status': 'queued'}
        self.queue.put((file_id, audio_path, callback))

        return file_id

    def get_status(self, file_id: str) -> dict:
        """Get transcription status"""
        return self.results.get(file_id, {'status': 'unknown'})

    def shutdown(self):
        """Shutdown all workers"""
        for _ in self.workers:
            self.queue.put(None)
        for worker in self.workers:
            worker.join()
```

#### Processing Strategy:
```python
def process_folder_optimized(folder: Path):
    """
    Optimized folder processing with audio prioritization.
    """
    # Phase 1: Scan for files
    print("üìÇ Scanning folder...")
    audio_files = []
    other_files = []

    for file in folder.rglob('*'):
        if file.is_file():
            if file.suffix.lower() in ['.mp3', '.m4a', '.wav', '.wma']:
                audio_files.append(file)
            else:
                other_files.append(file)

    print(f"  Found {len(audio_files)} audio files")
    print(f"  Found {len(other_files)} other files")

    # Phase 2: Queue audio files for background processing
    if audio_files:
        print("\nüéµ Queueing audio files for transcription...")
        queue = AudioTranscriptionQueue(max_workers=2)

        audio_jobs = {}
        for audio_file in audio_files:
            file_id = queue.add_audio_file(audio_file)
            audio_jobs[file_id] = audio_file
            print(f"  Queued: {audio_file.name}")

    # Phase 3: Process other files while audio transcribes
    print("\nüìÑ Processing other files...")
    for file in other_files:
        process_file(file)

        # Check if any audio files completed
        if audio_files:
            for file_id in list(audio_jobs.keys()):
                status = queue.get_status(file_id)
                if status['status'] == 'complete':
                    print(f"  [Audio] {audio_jobs[file_id].name} transcribed ‚úì")
                    # Process the transcript through LLM cleanup
                    process_transcript(status['transcript'])
                    del audio_jobs[file_id]

    # Phase 4: Wait for remaining audio files
    if audio_files:
        print("\n‚è≥ Waiting for remaining audio transcriptions...")
        queue.queue.join()
        queue.shutdown()
```

#### Expected Improvement:
- Large audio files: 10+ minutes ‚Üí 3-4 minutes (parallel chunking)
- Folder processing: Much faster (audio runs in background)
- User experience: Progress indicators, no waiting
- Reliability: Smaller chunks = less chance of timeout

---

### 3. Scanned PDFs

#### Current Flow:
```
PDF ‚Üí Ghostscript ‚Üí PNG images ‚Üí pytesseract ‚Üí Text
```

#### Optimized Flow:
```
PDF
  ‚Üì
Ghostscript (high res: 300 DPI)
  ‚Üì
For each page image:
  - Auto-enhance (contrast, B&W)
  - Deskew if needed
  - Remove noise
  ‚Üì
pytesseract OCR with language hints
  ‚Üì
Combine pages with page markers
  ‚Üì
LLM cleanup
```

#### Implementation:
- Use same image preprocessing as above
- Add language detection: `tesseract --psm 0` (OSD mode)
- Use Afrikaans model when detected: `tesseract -l afr+eng`

---

## üéØ Priority Implementation Order

### Phase 1: Quick Wins (Do First)
1. ‚úÖ **Document parsing** (DONE - use Unstructured)
2. **Image preprocessing** - 2-3 hours to implement, huge quality boost
3. **Audio format normalization** - 1 hour to implement, prevents conversion errors

### Phase 2: Performance (Do Next)
4. **Audio chunking** - 3-4 hours to implement, 3x faster on large files
5. **Parallel audio transcription** - 2-3 hours, another 2x speedup
6. **Progress indicators** - 1 hour, better UX

### Phase 3: Advanced (Do Later)
7. **Background queue system** - 4-5 hours, best UX
8. **Advanced OCR** (deskewing, noise removal) - 3-4 hours
9. **Automatic language detection** - 2-3 hours

---

## üìä Performance Comparison

### Current State:
- 50 MB audio file: 15 minutes sequential
- Low contrast image: 70% OCR accuracy
- Scanned PDF (20 pages): 10 minutes
- **Total folder processing: Sequential (slow)**

### After Optimizations:
- 50 MB audio file: 4 minutes (parallel chunks)
- Enhanced image: 95% OCR accuracy
- Scanned PDF (20 pages): 5 minutes (enhanced OCR)
- **Total folder processing: Parallel (fast)**

---

## üîß Implementation Files to Create

1. **`aster/preprocessors/image_enhancer.py`**
   - Image preprocessing for OCR
   - Contrast, sharpness, B&W conversion
   - Deskewing, noise removal

2. **`aster/preprocessors/audio_chunker.py`**
   - Audio file chunking with ffmpeg
   - Duration detection
   - Format normalization

3. **`aster/workers/audio_transcriber.py`**
   - Parallel transcription
   - Chunk reassembly
   - Timestamp formatting

4. **`aster/queue/background_jobs.py`**
   - Background job queue
   - Worker threads
   - Status tracking

5. **`aster/strategies/folder_optimizer.py`**
   - Smart folder processing
   - Audio prioritization
   - Parallel execution

---

## üí° Key Insights

1. **Preprocessing is critical** - Clean input = better output
2. **Parallel processing wins** - Don't wait sequentially
3. **Audio is the bottleneck** - Queue it first, process while doing other work
4. **Chunking prevents timeouts** - Smaller pieces = more reliable
5. **Progress feedback matters** - User experience is important

---

## üöÄ Next Steps

1. **Test current optimizations** (Unstructured for docs)
2. **Implement image preprocessing** (biggest quality win)
3. **Add audio chunking** (biggest speed win)
4. **Add progress indicators** (best UX win)
5. **Background queue** (ultimate workflow optimization)

Would you like me to implement any of these optimizations now?
